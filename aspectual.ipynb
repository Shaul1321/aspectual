{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aspectual",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTVfzuBUfklE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install libmysqlclient-dev\n",
        "!pip install pattern\n",
        "\n",
        "import urllib\n",
        "import spacy\n",
        "from typing import List, Tuple, Dict\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "from pattern.en import conjugate, PRESENT, PROGRESSIVE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZezzuyEbfnGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wiki(url = \"http://nlp.biu.ac.il/~ravfogs/neural_decomposition/wiki.raw.250k\"):\n",
        "    \n",
        "  response = urllib.request.urlopen(url)\n",
        "  raw = response.read().decode('utf8')\n",
        "  sents = raw.split(\"\\n\")\n",
        "  \n",
        "  return sents\n",
        "\n",
        "\n",
        "def create_docs(sentences: List[str]) -> List[spacy.tokens.Doc]:\n",
        "\n",
        "    #parsing and toekenizing (NOTE: takes about 10 minutes)\n",
        "    \n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    nlp.remove_pipe(\"ner\")\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    docs = list(nlp.pipe(sentences, batch_size = 100))\n",
        "    \n",
        "    docs = list(docs)\n",
        "    print(time.time() - start)\n",
        "    \n",
        "    return docs\n",
        "  \n",
        "\n",
        "def count_obj_verb_cooccurrences(docs: List[spacy.tokens.Doc]) -> Dict[str, Counter]:\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "    docs: a list of docs, each representing one sentence.\n",
        "  -------------------------------\n",
        "  Return:\n",
        "    counter: a Counter that counts co-occurrences of (direct object, verb). e.g., counter[\"book\"][\"read\"] counts co-occurrences of 'book' as the direct\n",
        "    object of 'read', as in the sentence 'I read a book. \n",
        "    Note that the verbs are conjugated to their lemma form.'\n",
        "  \"\"\"\n",
        "  counter = defaultdict(Counter)\n",
        "  for doc in tqdm(docs, total = len(docs)):\n",
        "    \n",
        "      for tok in doc:\n",
        "        \n",
        "        if tok.dep_ == \"dobj\":\n",
        "          parent = tok.head\n",
        "          obj_word_lemma, verb_word_lemma = tok.lemma_, parent.lemma_\n",
        "          counter[obj_word_lemma][verb_word_lemma] += 1\n",
        "  \n",
        "  return counter\n",
        "\n",
        "\n",
        "def process_one_sentence(sentence: str, counter: defaultdict) -> Tuple[List[str], bool, int]:\n",
        "  \n",
        "  \"\"\"\n",
        "  analyze one sentence, and try to add a verb if possible. \n",
        "  --------------------\n",
        "  Arguments:\n",
        "    sentence: a string. The sentence is assumed not to be of the easy class (i.e., no sentences like \"begin reading the book\")\n",
        "    counter: the Counter for (dobj, verb co-occurrences)\n",
        "  --------------------\n",
        "  Return:\n",
        "  \n",
        "    a tuple (new_sentence: List[str], modified: bool, index: int)\n",
        "            new_sentence: the sentence, possibly after the addition of a verb.\n",
        "            modified: a flag that is true if the sentence was modified, false otherwise.\n",
        "            index: the index of the added word. default: -1\n",
        "  \"\"\"\n",
        "  \n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  doc = nlp(sentence)\n",
        "  as_list = [tok.text for tok in doc]\n",
        "  \n",
        "  modified = False\n",
        "  \n",
        "  for i, tok in enumerate(doc):\n",
        "    \n",
        "    \n",
        "    if tok.dep_ == \"dobj\":\n",
        "      \n",
        "      verb_lemma = tok.head.lemma_\n",
        "      if tok.text in counter:\n",
        "        \n",
        "        modified = True\n",
        "        possible_verbs = [verb for (verb, count) in counter[tok.lemma_].most_common(10)]\n",
        "        most_probable_verb = possible_verbs[0]\n",
        "        verb_progressive = conjugate(most_probable_verb, tense = PRESENT, aspect = PROGRESSIVE)\n",
        "        verb_index = tok.head.i\n",
        "        as_list.insert(verb_index + 1, verb_progressive)\n",
        "        return (as_list, True, verb_index + 1)\n",
        "      \n",
        "  return (as_list, False, -1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zUfnKu7f3pB",
        "colab_type": "code",
        "outputId": "2fdbd154-9981-4346-9ae8-61cd0753282e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sents = load_wiki()\n",
        "docs = create_docs(sents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "560.0931024551392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e-xDil_nHxX",
        "colab_type": "code",
        "outputId": "85b07c71-8d5c-4f63-c1d4-c6972cef9bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "obj2verb = count_obj_verb_cooccurrences(docs)\n",
        "print(\"Common verbs for the word 'book': {}\".format(obj2verb[\"book\"].most_common(5)))\n",
        "print(\"Common verbs for the word 'door': {}\".format(obj2verb[\"door\"].most_common(5)))\n",
        "print(\"Common verbs for the word 'building': {}\".format(obj2verb[\"building\"].most_common(5)))\n",
        "print(\"Common verbs for the word 'food': {}\".format(obj2verb[\"food\"].most_common(5)))\n",
        "print(\"Common verbs for the word 'initiative': {}\".format(obj2verb[\"initiative\"].most_common(5)))\n",
        "\n",
        "\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 250001/250001 [00:03<00:00, 73169.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Common verbs for the word 'book': [('write', 65), ('publish', 53), ('read', 15), ('author', 13), ('have', 9)]\n",
            "Common verbs for the word 'door': [('open', 31), ('close', 17), ('show', 4), ('have', 3), ('leave', 2)]\n",
            "Common verbs for the word 'building': [('design', 10), ('include', 9), ('have', 8), ('construct', 8), ('purchase', 6)]\n",
            "Common verbs for the word 'food': [('eat', 7), ('prepare', 6), ('provide', 6), ('buy', 4), ('grow', 4)]\n",
            "Common verbs for the word 'initiative': [('take', 7), ('support', 5), ('promote', 4), ('announce', 2), ('have', 1)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zkibryWoJBR",
        "colab_type": "code",
        "outputId": "2c4e35e1-53ce-4d5f-9898-c6f398f066be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "aspectual = \"I just begun the book.\"\n",
        "aspectual2 = \"I continue the movie\"\n",
        "non_aspectual = \"I throw the book.\"\n",
        "non_aspectual2 = \"I started an initiative.\"\n",
        "non_aspectual3 = \"They continued their journey to Paris.\"\n",
        "print(process_one_sentence(aspectual, obj2verb))\n",
        "print(process_one_sentence(aspectual2, obj2verb))\n",
        "print(process_one_sentence(non_aspectual, obj2verb))\n",
        "print(process_one_sentence(non_aspectual2, obj2verb)) # grammatical but awkward -> probably higher perplexity\n",
        "print(process_one_sentence(non_aspectual3, obj2verb)) # this is grammatical, but doesn't sound so good. Maybe higher perplexity?\n",
        "\n"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['I', 'just', 'begun', 'writing', 'the', 'book', '.'], True, 3)\n",
            "(['I', 'continue', 'making', 'the', 'movie'], True, 2)\n",
            "(['I', 'throw', 'writing', 'the', 'book', '.'], True, 2)\n",
            "(['I', 'started', 'taking', 'an', 'initiative', '.'], True, 2)\n",
            "(['They', 'continued', 'making', 'their', 'journey', 'to', 'Paris', '.'], True, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyyTchAKzVj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
